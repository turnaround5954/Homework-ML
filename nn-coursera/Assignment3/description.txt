In this assignment, you're going to train a simple Neural Network, for recognizing handwritten digits. 
You'll be programming, looking into efficient optimization, and looking into effective regularization.

The dataset for this assignment is the USPS collection of handwritten digits. 
It consists of scans (images) of digits that people wrote. 
The input is a 16 by 16 image of greyscale pixels, showing an image of a handwritten digit. 
The output is simply which of the 10 different digits it is, so we're using a 10-way softmax as the output layer of our Neural Network. 
The input layer is simply 256 units, i.e. one for each pixel. 
We use one hidden layer of logistic units. 
One of the issues we'll be investigating is what number of hidden units works best for generalization. 
To keep things as simple as possible, we're not including biases in our model. 
In the diagram you can see that this model is significantly simpler than the model that we used in programming assignment 2.

In this assignment, we're mostly interested in the cross-entropy error, as opposed to the classification error rate. 
The reason for that is that the cross-entropy error is continuous and behaves better than the classification error rate. 
Only at the very end will we look at the classification error rate.
To investigate generalization, we need a training set, a validation set, and a test set, so the dataset has been split in 3 groups. 
We train our networks on the training set; 
we use the validation set to find out what's generalizing well and what isn't; 
and after we've made our choice of regularization strategy, we'll see how well our model performs on the test set. 
Those three subsets have already been made for you, and you're not expected to change them (you're not even allowed to change them). 
The full USPS dataset has 11,000 images. 
We're using 1,000 of them as training data, another 1,000 as validation data, and the remaining 9,000 as test data. 
Normally, one would use most of the data as training data, but for this assignment we'll use less, so that our programs run more quickly.
Before we get to the issue of generalization, we need a good optimization strategy. 
The optimizer that we're using is gradient descent with momentum, but we'll need to find good values for the learning rate and the momentum multiplier.

Part 1: Setting up
We're using Octave. You probably already have that working well, because you also needed it for assignment 2. Download the code fromhere, and the data from here. 
Make sure that the code file is called "a3.m" and the data file is called "data.mat". 
Place both of them in the same directory, start Octave, cd to that directory, and run a test run without any training: a3(0, 0, 0, 0, 0, false, 0). 
You should see messages that tell you the loss and classification error rate without any training.

Part 2: Programming
Most of the code has already been written for you. 
The script in a3.m loads the data (training, validation, and test), performs the optimization, and reports the results, 
including some numbers and a plot of the training data and validation data loss as training progresses. 
For the optimization it needs to be able to compute the gradient of the loss function, and that part is up to you to implement, in function d_loss_by_d_model. 
You're not allowed to change any other part of the code. 
However, you should take a quick look at it, and in particular you should make sure that you understand the meaning of the various parameters that the main script takes (see line 1 of a3.m).
The program checks your gradient computation for you, using a finite difference approximation to the gradient. 
If that finite difference approximation results in an approximate gradient that's very different from what your gradient computation procedure produced, then the program prints an error message. 
This is hugely helpful debugging information. Imagine that you have the gradient computation done wrong, but you don't have such a sanity check: 
your optimization would probably fail in many weird and wonderful ways, and you'd be worrying that perhaps you picked a bad learning rate or so. 
(In fact, that's exactly what happened to me when I was preparing this assignment, before I had the gradient checker working.) 
With a finite difference gradient checker, at least you'll know that you probably got the gradient right. It's all approximate, 
so the checker can never know for sure that you did it right, but if your gradient computation is seriously wrong, the checker will probably notice.
Take a good look at the loss computation, and make sure that you understand it.
Notice that there's classification loss and weight decay loss, which are added together to make the loss.
Also notice that the loss function is an average over training cases, as opposed to a sum. Of course, that affects the gradient as well.
Now take a pen & paper, figure out how to compute the gradient, and implement it in Octave.
Here are some step-by-step suggestions, but you don't need to use them, as long as you get that gradient computation right.
After you figured out the math on paper, start with the weight decay loss, and do a run with huge weight decay, so that the weight decay loss overshadows the classification loss. Run a3(1e7, 7, 10, 0, 0, false, 4), i.e. 7 hidden units. If the gradient check passes, then you probably did this right. If it doesn't, take a close look at the error message, and try to figure out where you may have made a mistake.
After you have the weight decay loss gradient working, turn off weight decay, i.e. go to a3(0, 7, 10, 0, 0, false, 4), and you'll see the gradient error message coming back.
Then implement the classification loss gradient, and if you get any error message from the gradient checker, look closely at the numbers in that error message. When you have a correct implementation, proceed to the next part.

Part 3: Optimization
We'll start with a small version of the task, to best see the effect of the optimization parameters. 
The small version is that we don't use weight decay or early stopping, we use only 10 hidden units, 70 optimization iterations, and mini-batches of size 4 
(usually, mini-batch size is more like 100, but for now we use 4).
While we're investigating how the optimization works best, we'll only look at the loss on training data. 
That's what we're directly optimizing, so if that gets low, then the optimizer did a good job, regardless whether the solution generalizes well to the validation data.
Let's do an initial run with with learning rate 0.005 and no momentum: run a3(0, 10, 70, 0.005, 0, false, 4)

Part 4: Generalization
Now that we found good optimization settings, we're switching to a somewhat bigger task, and there we'll investigate generalization. 
Now we're interested mostly in the classification loss on the validation data: 
if that's good, then we have good generalization, regardless whether the loss on the training data is small or large. 
Notice that we're measuring only the classification loss: 
we're not interested in the weight decay loss. The classification loss is what shows how well we generalize. 
When we don't use weight decay, the classification loss and the final loss are the same, because the weight decay loss is zero. 
We'll start with zero weight decay, 200 hidden units, 1000 optimization iterations, a learning rate of 0.35, momentum of 0.9, no early stopping, 
and mini-batch size 100, i.e. run a3(0, 200, 1000, 0.35, 0.9, false, 100). This run will take more time.